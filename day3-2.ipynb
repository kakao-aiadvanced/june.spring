{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba5d68a-d340-41b5-bbe1-60dd64bb74b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/june/Desktop/myvenv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model='llama3', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ed0fe86-f42e-4fa3-8d3c-3ee6e9ef7f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bc2f3e5-9173-4c8a-a4c0-dacee3917ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the API key\n",
    "os.environ['OPENAI_API_KEY'] = \"sk-\"\n",
    "os.environ['TAVILY_API_KEY'] = \"tvly-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9c16b03-51d0-4b08-83c9-93f01821bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain_text_splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67650d61-b944-4d40-8275-e0419fb2d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9904c7cf-d69b-4701-b0a9-a2f54bad16fd",
   "metadata": {},
   "source": [
    "## Todo\n",
    "0. vector store에 데이터 저장 [v]\n",
    "1. 유저로부터 Query 받기 [v]\n",
    "2. 쿼리로 retrieval 얻기 [v]\n",
    "3. retrieval 관련성 체크 [v]\n",
    "4. !통과 못한 경우 웹서치 [v]\n",
    "5. !web search 관련성 체크 [v]\n",
    "6. 통과한 경우 RAG + query를 넣어서 답변 [v]\n",
    "7. 할루시네이션 체크 (근거가 있는 사실인지 물어보기) [v]\n",
    "8. 할루시네이션이 아닌 경우 바로 답변 [v]\n",
    "9. 할루시네이션인 경우 다시 답변 생성 (retry 5 times) [v]\n",
    "10. !답변에 제목 및 출처 추가 [v]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc114bff-3bf2-4d3f-a3bf-d53c735b141b",
   "metadata": {},
   "source": [
    "## Split and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e139e95-491a-48a9-9d53-50eea493ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a sample vectorDB\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "all_splits = []\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# VectorDB\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(persist_directory=\"./chroma_db\", documents=doc_splits, embedding=embedding)\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57f7545-8f46-4329-9e5f-c438bbb71b6d",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "987067b9-de9d-4969-a712-9485888fb0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# question = \"agent memory\"\n",
    "# retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "#     retriever=vectordb.as_retriever(), llm=llm\n",
    "# )\n",
    "\n",
    "# rags = retriever_from_llm.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a942a7c-2ef8-4221-9d49-78b838dd2600",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM\n",
    "\n",
    "local_llm = \"llama3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510bce47-1cfb-4d22-a495-63efe4ea189d",
   "metadata": {},
   "source": [
    "## Relevance Check - LLM as a Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b95f799-2d26-45fe-8cc5-0cb71a9baf30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 0.05}\n",
      "[Document(page_content='follows:', metadata={'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\"}), Document(page_content='follows:', metadata={'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\"}), Document(page_content='follows:', metadata={'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\"}), Document(page_content='follows:', metadata={'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\"})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "            template = \"\"\"\n",
    "            <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "                \n",
    "            You are developing a retrieval(context) evaluator system. \n",
    "            Define the criteria that determine if a retrieved document is relevant to a user's question. \n",
    "            Your goal is to create a set of guidelines that the system will follow to assess relevance accurately. \n",
    "            First, consider the key elements that indicate relevance between a user's question and a retrieved document. \n",
    "            Think about how the system can analyze the content to make this determination effectively.\n",
    "            Please ANSWER TOTOAL SCORE IN FORMAT 'answer:float' whch has key named answer and value of score.\n",
    "            <|eot_id|>\n",
    "            <|start_header_id|>user<|end_header_id|>\n",
    "            Context : {context}\n",
    "            User Question : {question}<|eot_id|>\n",
    "            <|start_header_id|>assistant<|end_header_id|>\n",
    "            \"\"\",\n",
    "            input_variables=[\"question\", \"context\"],\n",
    "        )\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"Who are the Bears expected to draft first in the NFL draft?\"\n",
    "docs = retriever.invoke(question)\n",
    "\n",
    "print(retrieval_grader.invoke({\"question\": question, \"context\": docs[0].metadata}))\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84673e46-e182-44ba-b822-9a202013c33e",
   "metadata": {},
   "source": [
    "## RAG + query로 질문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a5fecaa-02b4-4cc0-9de0-672b81e9e965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, it appears that you are asking about the concept of an \"agent memory\" in the context of LLM-powered autonomous agents.\n",
      "\n",
      "From the given text, I can infer that the \"Memory stream\" is a long-term memory module (external database) that records a comprehensive list of agents' experience in natural language. Each element in this memory stream is an observation or event directly provided by the agent. Additionally, inter-agent communication can trigger new natural language statements.\n",
      "\n",
      "In this context, the \"agent memory\" refers to the Memory Stream, which serves as a repository for storing and retrieving information about the agent's experiences, observations, and events. This memory stream plays a crucial role in informing the agent's behavior by surfacing relevant context based on factors such as relevance, recency, and importance.\n",
      "\n",
      "If you have any specific questions or would like further clarification on this topic, please feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "            template = \"\"\"\n",
    "            <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "            ADD TITLES AND URLS THAT IS USED FROM GIVEN CONTEXT.\n",
    "            Start by understanding the user's question.\n",
    "            Analyze the context to determine the appropriate response.\n",
    "            Tailor the response based on the context provided.\n",
    "            Provide helpful and relevant information to the user within the specified context.\n",
    "            <|eot_id|>\n",
    "            <|start_header_id|>user<|end_header_id|>\n",
    "            Context\n",
    "            {context}\n",
    "            '''\n",
    "            User Question\n",
    "            {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "            \"\"\",\n",
    "            input_variables=[\"question\", \"context\"],\n",
    "        )\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35808c-2c09-4f34-b23d-b0765762af1f",
   "metadata": {},
   "source": [
    "## Hallucination Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30d36208-a269-48fe-bbb7-286fec56604a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_hallucinated': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "            template = \"\"\"\n",
    "            <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "                \n",
    "            You are a hallucination checker AI. Your task is to assess whether a given answer generated by the LLM model corresponds to a user's question and determine if it exhibits signs of hallucination. \n",
    "            If the answer is deemed hallucinatory, provide guidance on how to identify and correct such responses. If the answer is coherent, acknowledge its clarity and relevance to the question.\n",
    "            \n",
    "            Take a deep breath and let's take this step by step.\n",
    "            \n",
    "            Analyze the user's question carefully.\n",
    "            Evaluate the LLM-generated answer in relation to the question.\n",
    "            Determine if the answer shows signs of hallucination or if it directly addresses the question.\n",
    "            If hallucination is detected, provide feedback on how to improve the response.\n",
    "            If the answer is coherent, acknowledge its relevance and clarity.\n",
    "            Please ANSWER TOTOAL SCORE IN JSON FORMAT 'is_hallucinated:boolean'.\n",
    "            <|eot_id|>\n",
    "            <|start_header_id|>user<|end_header_id|>\n",
    "            LLM-generated answer\n",
    "            {generation}\n",
    "            ''\n",
    "            User question\n",
    "            {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "            \"\"\",\n",
    "            input_variables=[\"generation\", \"question\"],\n",
    "        )\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "hallucination_grader.invoke({\"question\": question, \"generation\": generation})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f376bdd-a8f5-4aa5-9708-348426a5a569",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04f417ba-c856-47a0-bdac-090fecf601dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "import copy\n",
    "\n",
    "### State\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents\n",
    "        retry: retry count\n",
    "        useful: whether the result is useful\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n",
    "    retry: int\n",
    "\n",
    "\n",
    "### Nodes\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    \n",
    "    transformed_documents = []\n",
    "    for detailed_doc in documents:\n",
    "        metadata = detailed_doc.metadata\n",
    "        transformed_doc = {\n",
    "            'url': metadata.get('source', ''),\n",
    "            'title': metadata.get('title', 'Untitled Document'),\n",
    "            'content': detailed_doc.page_content\n",
    "        }\n",
    "        transformed_documents.append(transformed_doc)\n",
    "    \n",
    "    return {\"documents\": transformed_documents, \"question\": question, \"retry\": 0}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation, \"retry\": state[\"retry\"] + 1}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"context\": d}\n",
    "        )\n",
    "        grade = score[\"answer\"]\n",
    "        # Document relevant\n",
    "        if grade > 0.5:\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    transformed_documents = []\n",
    "    for doc in docs:\n",
    "        transformed_doc = {\n",
    "            'url': doc.get('url', ''),\n",
    "            'title': doc.get('title', 'Untitled Document'),\n",
    "            'content': doc.get('content', '')\n",
    "        }\n",
    "        transformed_documents.append(transformed_doc)\n",
    "    if documents is not None:\n",
    "        documents= copy.deepcopy(transformed_documents)\n",
    "    else:\n",
    "        documents = [web_results]\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if len(filtered_documents) == 0:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n",
    "        )\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "### Conditional edge\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"question\": question, \"generation\": generation}\n",
    "    )\n",
    "    is_hallucinated = score[\"is_hallucinated\"]\n",
    "\n",
    "    # Check hallucination\n",
    "    if state[\"retry\"] == 5:\n",
    "        print(\"---NOT ANSWERABLE---\")\n",
    "        return \"useful\"\n",
    "    elif is_hallucinated:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "        return \"useful\"\n",
    "\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"websearch\", web_search)  # web search\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ee3c16-9e43-4f1c-b50e-f6fe8af3f64f",
   "metadata": {},
   "source": [
    "## Graph Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c9196f1-3ffb-4991-916a-43d2c76e33f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"generate\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "000b3d0e-9ddb-444f-890d-9dfccb66c3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "'Finished running: retrieve:'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "'Finished running: grade_documents:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "'Finished running: generate:'\n",
      "(\"Based on the provided context, it appears that you're looking for \"\n",
      " 'information about the types of memory used in LLM-powered autonomous '\n",
      " 'agents. \\n'\n",
      " '\\n'\n",
      " 'According to the text, there are two main types of memory mentioned:\\n'\n",
      " '\\n'\n",
      " '1. **Short-term memory**: This type of memory is utilized through in-context '\n",
      " 'learning (Prompt Engineering) and allows the model to learn within a '\n",
      " 'specific context.\\n'\n",
      " '2. **Long-term memory**: This type of memory enables the agent to retain and '\n",
      " 'recall information over extended periods by leveraging an external vector '\n",
      " 'store and fast retrieval.\\n'\n",
      " '\\n'\n",
      " 'These two types of memory are crucial for LLM-powered autonomous agents, '\n",
      " 'allowing them to learn and adapt in real-time while also retaining knowledge '\n",
      " 'and experiences over time.\\n'\n",
      " '\\n'\n",
      " 'References:\\n'\n",
      " '* Lil\\'Log: \"LLM Powered Autonomous Agents\" '\n",
      " '(https://lilianweng.github.io/posts/2023-06-23-agent/)')\n"
     ]
    }
   ],
   "source": [
    "# # Compile\n",
    "# app = workflow.compile()\n",
    "\n",
    "# # Test\n",
    "\n",
    "# inputs = {\"question\": \"What are the types of agent memory?\"}\n",
    "# for output in app.stream(inputs):\n",
    "#     for key, value in output.items():\n",
    "#         pprint(f\"Finished running: {key}:\")\n",
    "# pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e58cae4-9720-4a72-8738-125d7dc6bedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "'Finished running: retrieve:'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\n",
      "'Finished running: grade_documents:'\n",
      "---WEB SEARCH---\n",
      "'Finished running: websearch:'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "'Finished running: grade_documents:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "'Finished running: generate:'\n",
      "('Based on the provided context, it appears that the Chicago Bears have '\n",
      " 'already made their selection for the first round of the 2024 NFL Draft. '\n",
      " 'According to the articles, they selected USC quarterback Caleb Williams with '\n",
      " 'the No. 1 overall pick.\\n'\n",
      " '\\n'\n",
      " 'Here are some relevant URLs and titles from the given context:\\n'\n",
      " '\\n'\n",
      " '* \"Chicago Bears\\' NFL draft winners: First-round expert analysis on Caleb '\n",
      " 'Williams and Rome Odunze\" - '\n",
      " 'https://bearswire.usatoday.com/lists/chicago-bears-nfl-draft-winners-first-round-expert-analysis-caleb-williams-rome-odunze/\\n'\n",
      " '* \"2024 Chicago Bears NFL Draft picks, news, photos, highlights\" - '\n",
      " 'https://www.chicagobears.com/news/2024-chicago-bears-nfl-draft-picks-news-photos-highlights\\n'\n",
      " '* \"Full list of Bears draft picks for the 2024 NFL Draft\" - '\n",
      " 'https://sports.yahoo.com/full-list-bears-draft-picks-143003997.html\\n'\n",
      " '\\n'\n",
      " 'As per these sources, Caleb Williams was widely considered the top prospect '\n",
      " 'in a draft class loaded with talented quarterbacks and was selected by the '\n",
      " 'Chicago Bears with the No. 1 pick.\\n'\n",
      " '\\n'\n",
      " 'Please note that since the selection has already been made, there is no '\n",
      " 'expectation of who the Bears will draft first; instead, we can provide '\n",
      " 'information on their actual selection.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()\n",
    "inputs = {\"question\": \"Who are the Bears expected to draft first in the NFL draft?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "if value[\"retry\"] != 5:\n",
    "    pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a05f82f2-4056-435d-ad89-63280a265ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "\n",
    "# # Compile\n",
    "# app = workflow.compile()\n",
    "# inputs = {\"question\": \"who is ceo of tesla?\"}\n",
    "# for output in app.stream(inputs):\n",
    "#     for key, value in output.items():\n",
    "#         pprint(f\"Finished running: {key}:\")\n",
    "# if value[\"retry\"] != 5:\n",
    "#     pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c048fd67-e04a-4dbb-a4dc-5aa31c9f0033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
