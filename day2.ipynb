{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba5d68a-d340-41b5-bbe1-60dd64bb74b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/june/Desktop/myvenv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "llm = ChatOllama(model='llama3', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ed0fe86-f42e-4fa3-8d3c-3ee6e9ef7f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bc2f3e5-9173-4c8a-a4c0-dacee3917ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the API key\n",
    "os.environ['OPENAI_API_KEY'] = \"sk-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9c16b03-51d0-4b08-83c9-93f01821bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain_text_splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67650d61-b944-4d40-8275-e0419fb2d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9904c7cf-d69b-4701-b0a9-a2f54bad16fd",
   "metadata": {},
   "source": [
    "## Todo\n",
    "0. vector store에 데이터 저장 [v]\n",
    "1. 유저로부터 Query 받기 [v]\n",
    "2. 쿼리로 retrieval 얻기 [v]\n",
    "3. 관련성 체크 (cosine similarity가 N이상인지?) [v]\n",
    "4. 통과 못한 경우 context에 없다고 답변 [v]\n",
    "5. 통과한 경우 RAG + query를 넣어서 답변 [v]\n",
    "6. 할루시네이션 체크 (근거가 있는 사실인지 물어보기) [v]\n",
    "7. 할루시네이션이 아닌 경우 바로 답변 [v]\n",
    "8. 할루시네이션인 경우 다시 답변 생성 (retry 5 times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc114bff-3bf2-4d3f-a3bf-d53c735b141b",
   "metadata": {},
   "source": [
    "## Split and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e139e95-491a-48a9-9d53-50eea493ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a sample vectorDB\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "all_splits = []\n",
    "\n",
    "for url in urls:\n",
    "    loader = WebBaseLoader(url)\n",
    "    data = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "    splits = text_splitter.split_documents(data)\n",
    "    all_splits.extend(splits)\n",
    "    \n",
    "    # VectorDB\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    vectordb = Chroma.from_documents(persist_directory=\"./chroma_db\", documents=all_splits, embedding=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57f7545-8f46-4329-9e5f-c438bbb71b6d",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "987067b9-de9d-4969-a712-9485888fb0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "question = \"agent memory\"\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(), llm=llm\n",
    ")\n",
    "\n",
    "rags = retriever_from_llm.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b1bd90-0ca2-4a97-885b-b4eb479d6502",
   "metadata": {},
   "source": [
    "## Relevance Check - Try 1 cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29d94e48-db33-49ac-aeab-dbe700864e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Step 1: Compute the dot product\n",
    "    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
    "\n",
    "    # Step 2: Compute the magnitudes of the vectors\n",
    "    magnitude_vec1 = math.sqrt(sum(a * a for a in vec1))\n",
    "    magnitude_vec2 = math.sqrt(sum(b * b for b in vec2))\n",
    "\n",
    "    # Step 3: Compute the cosine similarity\n",
    "    if magnitude_vec1 == 0 or magnitude_vec2 == 0:\n",
    "        # To avoid division by zero\n",
    "        return 0.0\n",
    "    return dot_product / (magnitude_vec1 * magnitude_vec2)\n",
    "\n",
    "def prompt_router(query, rags):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "    rag_texts = [doc.page_content for doc in rags]  # Using the 'page_content' attribute\n",
    "    rag_embeddings = embeddings.embed_documents(rag_texts) \n",
    "\n",
    "    similarities = [cosine_similarity(query_embedding, rag_embedding) for rag_embedding in rag_embeddings]\n",
    "\n",
    "    # Filter documents with similarity > 0.7\n",
    "    similar_documents = []\n",
    "    for i, sim in enumerate(similarities):\n",
    "        print(sim)\n",
    "        if sim > 0.8:\n",
    "            similar_documents.append(rags[i])  # Assuming rags is a list of Document objects\n",
    "\n",
    "    return similar_documents\n",
    "\n",
    "# similar_documents = prompt_router(question, rags)\n",
    "# print(similar_documents)\n",
    "# 어떻게 이 결과를 llm에 넘겨서 없다고 대답하게 하지.. \n",
    "# cosine similarity로는 관련도를 제대로 판단을 못하는거 같다 ㅠ.ㅠ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510bce47-1cfb-4d22-a495-63efe4ea189d",
   "metadata": {},
   "source": [
    "## Relevance Check - Try 2 \bLLM as a Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b95f799-2d26-45fe-8cc5-0cb71a9baf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "import re\n",
    "\n",
    "def extract_score_from_output(content):\n",
    "    pattern = r\"#Answer:\\s*(-?\\d+(?:\\.\\d+)?)\"\n",
    "    matches = re.findall(pattern, content)\n",
    "    \n",
    "    if matches:\n",
    "        try:\n",
    "            score = float(matches[0])\n",
    "            return score\n",
    "        except ValueError as e:\n",
    "            print(f\"Error converting score to float: {e}\")\n",
    "    return None\n",
    "\n",
    "template = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    \n",
    "You are developing a retrieval(context) evaluator system. \n",
    "Define the criteria that determine if a retrieved document is relevant to a user's question. \n",
    "Your goal is to create a set of guidelines that the system will follow to assess relevance accurately. \n",
    "First, consider the key elements that indicate relevance between a user's question and a retrieved document. \n",
    "Think about how the system can analyze the content to make this determination effectively.\n",
    "Consider the fact that retrieval will not be used when the score is below 0.5.\n",
    "Please ANSWER TOTOAL SCORE IN FORMAT '#Answer:float'.\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Context : {context}\n",
    "User Question : {question}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": itemgetter(\"context\"),\n",
    "    \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "filtered_rags = []\n",
    "for rag in rags:\n",
    "    output = rag_chain.invoke({\"context\": rag, \"question\": question})\n",
    "    score = extract_score_from_output(output.content)\n",
    "    if (score is not None and score > 0.5):\n",
    "        filtered_rags.extend(rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84673e46-e182-44ba-b822-9a202013c33e",
   "metadata": {},
   "source": [
    "## RAG + query로 질문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a5fecaa-02b4-4cc0-9de0-672b81e9e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    \n",
    "You are assistant who answers about given context.\n",
    "Answer \"no context\" if there isn't any given context.\n",
    "'''\n",
    "Context\n",
    "{context}\n",
    "''\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": itemgetter(\"context\"),\n",
    "    \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "output = rag_chain.invoke({\"context\": filtered_rags, \"question\": question})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35808c-2c09-4f34-b23d-b0765762af1f",
   "metadata": {},
   "source": [
    "## Hallucination Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30d36208-a269-48fe-bbb7-286fec56604a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given context, it seems that you are referring to the concept of agent memory in the context of LLM-powered autonomous agents.\n",
      "\n",
      "In this context, agent memory refers to the ability of an agent to retain and recall information over extended periods. This is different from short-term memory, which is used for learning and processing information in real-time.\n",
      "\n",
      "According to the text, long-term memory provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
      "\n",
      "In addition, the text mentions that LLM-powered autonomous agents combine LLM with memory, planning, and reflection mechanisms to enable agents to behave conditioned on past experience, as well as interact with other agents. This suggests that agent memory plays a crucial role in enabling these agents to learn from their experiences and adapt to new situations.\n",
      "\n",
      "Overall, agent memory is an important aspect of LLM-powered autonomous agents, allowing them to retain and recall information over time and use it to inform their decision-making processes.\n"
     ]
    }
   ],
   "source": [
    "def extract_boolean_from_output(content):\n",
    "    pattern = r\"#Answer:\\s*(True|False)\"\n",
    "    matches = re.findall(pattern, content, re.IGNORECASE)  # Using re.IGNORECASE to handle different cases like 'true' or 'false'\n",
    "    \n",
    "    if matches:\n",
    "        answer = matches[0].lower()\n",
    "        return True if answer == 'true' else False\n",
    "    return None\n",
    "\n",
    "template = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    \n",
    "You are a hallucination checker AI. Your task is to assess whether a given answer generated by the LLM model corresponds to a user's question and determine if it exhibits signs of hallucination. \n",
    "If the answer is deemed hallucinatory, provide guidance on how to identify and correct such responses. If the answer is coherent, acknowledge its clarity and relevance to the question.\n",
    "\n",
    "Take a deep breath and let's take this step by step.\n",
    "\n",
    "Analyze the user's question carefully.\n",
    "Evaluate the LLM-generated answer in relation to the question.\n",
    "Determine if the answer shows signs of hallucination or if it directly addresses the question.\n",
    "If hallucination is detected, provide feedback on how to improve the response.\n",
    "If the answer is coherent, acknowledge its relevance and clarity.\n",
    "Please answer \"#Answer:true\" if it is hallucinated otherwise \"#Answer:false\"\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "LLM-generated answer\n",
    "{answer}\n",
    "''\n",
    "User question\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"answer\": itemgetter(\"answer\"),\n",
    "    \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "final_output = rag_chain.invoke({\"answer\": output.content, \"question\": question})\n",
    "\n",
    "is_hallucinated = extract_boolean_from_output(final_output.content)\n",
    "if not is_hallucinated:\n",
    "    print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f417ba-c856-47a0-bdac-090fecf601dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
