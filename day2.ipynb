{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fba5d68a-d340-41b5-bbe1-60dd64bb74b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "llm = ChatOllama(model='llama3', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9ed0fe86-f42e-4fa3-8d3c-3ee6e9ef7f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7bc2f3e5-9173-4c8a-a4c0-dacee3917ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the API key\n",
    "# os.environ['OPENAI_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e9c16b03-51d0-4b08-83c9-93f01821bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain_text_splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "67650d61-b944-4d40-8275-e0419fb2d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9904c7cf-d69b-4701-b0a9-a2f54bad16fd",
   "metadata": {},
   "source": [
    "## Todo\n",
    "0. vector store에 데이터 저장 [v]\n",
    "1. 유저로부터 Query 받기 [v]\n",
    "2. 쿼리로 retrieval 얻기 [v]\n",
    "3. 관련성 체크 (cosine similarity가 N이상인지?) [v]\n",
    "4. 통과 못한 경우 context에 없다고 답변 [v]\n",
    "5. 통과한 경우 RAG + query를 넣어서 답변 [v]\n",
    "6. 할루시네이션 체크 (근거가 있는 사실인지 물어보기)\n",
    "7. 할루시네이션이 아닌 경우 바로 답변\n",
    "8. 할루시네이션인 경우 다시 답변 생성 (retry 5 times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc114bff-3bf2-4d3f-a3bf-d53c735b141b",
   "metadata": {},
   "source": [
    "## Split and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5e139e95-491a-48a9-9d53-50eea493ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a sample vectorDB\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "all_splits = []\n",
    "\n",
    "for url in urls:\n",
    "    loader = WebBaseLoader(url)\n",
    "    data = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "    splits = text_splitter.split_documents(data)\n",
    "    all_splits.extend(splits)\n",
    "    \n",
    "    # VectorDB\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    vectordb = Chroma.from_documents(persist_directory=\"./chroma_db\", documents=all_splits, embedding=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57f7545-8f46-4329-9e5f-c438bbb71b6d",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "987067b9-de9d-4969-a712-9485888fb0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "question = \"agent memory\"\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(), llm=llm\n",
    ")\n",
    "\n",
    "rags = retriever_from_llm.invoke(question)\n",
    "len(rags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b1bd90-0ca2-4a97-885b-b4eb479d6502",
   "metadata": {},
   "source": [
    "## Relevance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "29d94e48-db33-49ac-aeab-dbe700864e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7727910001943669\n",
      "0.7661705053070712\n",
      "0.7979500562752266\n",
      "0.7510829761761391\n",
      "0.7522405097294081\n",
      "0.7299772381868517\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Step 1: Compute the dot product\n",
    "    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
    "\n",
    "    # Step 2: Compute the magnitudes of the vectors\n",
    "    magnitude_vec1 = math.sqrt(sum(a * a for a in vec1))\n",
    "    magnitude_vec2 = math.sqrt(sum(b * b for b in vec2))\n",
    "\n",
    "    # Step 3: Compute the cosine similarity\n",
    "    if magnitude_vec1 == 0 or magnitude_vec2 == 0:\n",
    "        # To avoid division by zero\n",
    "        return 0.0\n",
    "    return dot_product / (magnitude_vec1 * magnitude_vec2)\n",
    "\n",
    "def prompt_router(query, rags):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "    rag_texts = [doc.page_content for doc in rags]  # Using the 'page_content' attribute\n",
    "    rag_embeddings = embeddings.embed_documents(rag_texts) \n",
    "\n",
    "    similarities = [cosine_similarity(query_embedding, rag_embedding) for rag_embedding in rag_embeddings]\n",
    "\n",
    "    # Filter documents with similarity > 0.7\n",
    "    similar_documents = []\n",
    "    for i, sim in enumerate(similarities):\n",
    "        print(sim)\n",
    "        if sim > 0.8:\n",
    "            similar_documents.append(rags[i])  # Assuming rags is a list of Document objects\n",
    "\n",
    "    return similar_documents\n",
    "\n",
    "similar_documents = prompt_router(question, rags)\n",
    "print(similar_documents)\n",
    "# 어떻게 이 결과를 llm에 넘겨서 없다고 대답하게 하지.. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84673e46-e182-44ba-b822-9a202013c33e",
   "metadata": {},
   "source": [
    "## RAG + query로 질문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9a5fecaa-02b4-4cc0-9de0-672b81e9e965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='No context.' response_metadata={'model': 'llama3', 'created_at': '2024-06-18T08:07:56.329883Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 354902708, 'load_duration': 3375875, 'prompt_eval_count': 58, 'prompt_eval_duration': 292145000, 'eval_count': 4, 'eval_duration': 57440000} id='run-f45bfd4c-0dee-4fa9-bf6e-29cedf55eb68-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "def documents_to_dicts(documents):\n",
    "    result = []\n",
    "    for doc in documents:\n",
    "        doc_dict = {\n",
    "            \"page_content\": doc.page_content,\n",
    "            \"metadata\": {\n",
    "                \"description\": doc.metadata.get('description', ''),\n",
    "                \"language\": doc.metadata.get('language', ''),\n",
    "                \"source\": doc.metadata.get('source', ''),\n",
    "                \"title\": doc.metadata.get('title', '')\n",
    "            }\n",
    "        }\n",
    "        result.append(doc_dict)\n",
    "    return result\n",
    "\n",
    "# # Decomposition\n",
    "template = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    \n",
    "You are assistant who answers about given context.\n",
    "Answer \"no context\" if there isn't any given context.\n",
    "'''\n",
    "Context\n",
    "{context}\n",
    "'''\n",
    "#Answer [list of numbers]\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "context_text = \"\\n\\n\\n\".join(doc.page_content for doc in similar_documents)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": itemgetter(\"context\"),\n",
    "    \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "output = rag_chain.invoke({\"context\": similar_documents, \"question\": question})\n",
    "\n",
    "print(output)\n",
    "\n",
    "# rag_chain.invoke(\"What is Task Decomposition?\")\n",
    "\n",
    "\n",
    "# # LLM\n",
    "# llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# # Chain\n",
    "# generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# # Run\n",
    "# question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "# questions = generate_queries_decomposition.invoke({\"question\":question})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35808c-2c09-4f34-b23d-b0765762af1f",
   "metadata": {},
   "source": [
    "## Hallucination Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d36208-a269-48fe-bbb7-286fec56604a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
